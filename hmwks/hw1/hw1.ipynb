{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "45f3e6e2a4f3c118c492e2e664186b71",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 1: Decision tree, K-nearest neighbor, and the Bias-Variance Trade-Off\n",
    "\n",
    "\n",
    "\n",
    "This assignment is due on Moodle by **11:59pm on Friday Sep 14**. \n",
    "Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.\n",
    "Your solutions to computational questions should include any specified Python code and results \n",
    "as well as written commentary on your conclusions.\n",
    "Remember that you are encouraged to discuss the problems with your instructors and classmates, \n",
    "but **you must write all code and solutions on your own**. For a refresher on the course **Collaboration Policy** click [here](https://github.com/BoulderDS/CSCI-4622-Machine-Learning-18fa/blob/master/info/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. \n",
    "\n",
    "**Acknowledgment**: Noah Smith, Chris Ketelsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please put your name and cuidentity key.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ff7d53666cb767a1719373f5927a8ebe",
     "grade": false,
     "grade_id": "load",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "648d70f413d05fe87e969505d6fe54ca",
     "grade": false,
     "grade_id": "1intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### [30 points] Problem 1 - Decision tree\n",
    "***\n",
    "\n",
    "Consider the problem of predicting whether a person has a college degree based on age, salary, and Colorado residency. \n",
    "The dataset looks like the following.\n",
    "\n",
    "| Age   | Salary         | Colorado Residency      | College degree| \n",
    "|:------:|:------------:| :-----------:|---:|\n",
    "| 24 | 40,000 | Yes | Yes |\n",
    "| 53 | 52,000 | No | No |\n",
    "| 23 | 25,000 | Yes | No |\n",
    "| 25 | 77,000 | Yes | Yes |\n",
    "| 32 | 48,000 | No | Yes |\n",
    "| 52 | 110,000 | Yes | Yes |\n",
    "| 22 | 38,000 | Yes | Yes |\n",
    "| 43 | 44,000 | Yes | No |\n",
    "| 52 | 27,000 | No | No |\n",
    "| 48 | 65,000 | Yes | Yes |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f6d8fac06f21901f7fc07fe3546ae047",
     "grade": false,
     "grade_id": "1q",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Part 1 [5 points]**: Convert the above table to data. Two variables should be created:\n",
    "        \n",
    "1. $x$ is a $10*3$ matrix that contains the data from columns 0, 1, and 2. Colorado residency is represented by 1 (yes) and 0 (no).\n",
    "2. $y$ contains the labels (college degree), 1 (yes) and 0 (no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "99d49c4279d566943cb0e9e1eed98c4c",
     "grade": true,
     "grade_id": "1a",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b2941a4632161ae3fe6a2f752705c717",
     "grade": false,
     "grade_id": "2q1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Part 2 [16 points]:** Criteria for choosing a feature to split.\n",
    "\n",
    "**[2 points]** We start with no splitting. Assuming that our algorithm is deterministic, what is the smallest number of mistakes we can make if we do not use any of the features and what is the algorithm? (**Write your answer in the Markdown cell below.**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "483fd69f569c413806fa39b054f7baea",
     "grade": true,
     "grade_id": "2a1",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "52016b05960712f821c877830daba3ab",
     "grade": false,
     "grade_id": "2q2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**[5 points]** We start by considering the variable *Colorado residency*. The first criteria is based on the numbeer of mistakes. We need to build a confusion matrix between Colorado residency and college degree.\n",
    "\n",
    "How many mistakes will we make if we split based on Colorado residency? (**Answer below by finishing the code.**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "5f454b81086a6ca29e57080641740b00",
     "grade": false,
     "grade_id": "2a2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_error_in_leaf(y, ids):\n",
    "    \"\"\"\n",
    "    Returns the errors in a leaf node of a decision tree.\n",
    "    This function can be used to answer the previous question automatically.\n",
    "    \n",
    "    :@param y: all labels\n",
    "    :@param ids: the subset of indexes in the leaf node\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def error_criteria(y, root, left_child, right_child):\n",
    "    \"\"\"\n",
    "    Returns the number of errors if we split the root into the left child and the right child.\n",
    "    \n",
    "    :@param y: all labels\n",
    "    :@param root: indexes of all the data points in the root\n",
    "    :@param left_child: the subset of indexes in the left child\n",
    "    :@param right_child: the subset of indexes in the right child\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def value_split_binary_feature(x, y, fid, root, criteria_func):\n",
    "    left_child = [i for i in root if x[i, fid] == 0]\n",
    "    right_child = [i for i in root if x[i, fid] == 1]\n",
    "    return criteria_func(y, root, left_child, right_child)\n",
    "\n",
    "# Colorado residency should correpsond to the third column in your data x\n",
    "fid = 2\n",
    "root = list(range(len(y))) # root includes all data points\n",
    "mistakes = value_split_binary_feature(x, y, fid, root, error_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d81459206763a15d4cf3778019809ab7",
     "grade": true,
     "grade_id": "cell-dd1ff1e66981cc11",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "784e0eb1ff6781d5784c9353c7a449d2",
     "grade": false,
     "grade_id": "2a3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**[3 points]** Alternatively, we can use information gain to split the data. To get you familiar with MathJax, please write the equation necessary to compute information gain if we split data $D$ into $D_1$ and $D_2$. **Write your answer in the Markdown cell below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cc02ae4ac9f958fd6b8221f0f8b9e4dc",
     "grade": true,
     "grade_id": "cell-f6f2bbd3029c7b93",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "275c411d60bd1c6d9e8f69a28f251b6b",
     "grade": false,
     "grade_id": "cell-b90188ff50c2eaf9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**[6 points]** Now we write a function for computing information gain. Use log2 for entropy computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "541d819cc9062a2c340bc34a52e77397",
     "grade": false,
     "grade_id": "cell-bb989c485308fcc4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def entropy(y, ids):\n",
    "    \"\"\"\n",
    "    Returns the entropy in the labels for the data points in ids.\n",
    "    \n",
    "    :@param y: all labels\n",
    "    :@param ids: the indexes of data points\n",
    "    \"\"\"\n",
    "    if len(ids) == 0: # deal with corner case when there is no data point.\n",
    "        return 0\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "def information_gain_criteria(y, root, left_child, right_child):\n",
    "    \"\"\"\n",
    "    Returns the information gain by splitting root into left child and right child.\n",
    "    \n",
    "    :@param y: all labels\n",
    "    :@param root: indexes of all the data points in the root\n",
    "    :@param left_child: the subset of indexes in the left child\n",
    "    :@param right_child: the subset of indexes in the right child\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "fid = 2\n",
    "root = list(range(len(y))) # root includes all data points\n",
    "info_gain = value_split_binary_feature(x, y, fid, root, information_gain_criteria)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "658e7df9c6dd5157e157ccfe2112ccc7",
     "grade": true,
     "grade_id": "cell-1059c1f151862da1",
     "locked": true,
     "points": 6,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "58634997a922d2a2e61024bf255b938a",
     "grade": false,
     "grade_id": "cell-7a340274d5eccbf2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Part 3 [9 points]**: Deal with continuous features.\n",
    "    \n",
    "**[6 points]** One way to deal with continuous (or ordinal) data is to define binary features based on thresholding of continuous features like Age and Salary.\n",
    "For example, you might convert ages to 0 if age is less than or equal to 50 and 1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "af537c67335d4dfb985858a14becafe1",
     "grade": false,
     "grade_id": "cell-94945cb8ee7f6b14",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def value_split_continuous_feature(x, y, fid, root, criteria_func=information_gain_criteria):\n",
    "    \"\"\"\n",
    "    Return the best value and its corresponding threshold by splitting based on a continuous feature.\n",
    "\n",
    "    :@param x: all feature values\n",
    "    :@param y: all labels\n",
    "    :@param fid: feature id to split the tree based on\n",
    "    :@param root: indexes of all the data points in the root\n",
    "    :@param criteria_func: the splitting criteria function\n",
    "    \"\"\"\n",
    "    best_value, best_thres = 0, 0\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return best_value, best_thres\n",
    "\n",
    "root = list(range(len(y))) # root includes all data points\n",
    "fid = 0\n",
    "age_value, age_thres = value_split_continuous_feature(x, y, fid, root, information_gain_criteria)\n",
    "fid = 1\n",
    "salary_value, salary_thres = value_split_continuous_feature(x, y, fid, root, information_gain_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5207ad3b8e16b5567c83e5c810dc7fda",
     "grade": true,
     "grade_id": "cell-b073fa94cd720ea6",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "12fbe5a86aee427f5a5197467b7496c4",
     "grade": true,
     "grade_id": "cell-4fca1b5988b02f7a",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e450d10bdb905a2efed928dfd7335711",
     "grade": false,
     "grade_id": "cell-c7507cb413cfd74c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**[3 points]** Based on the current information gain by splitting different features, if we build a decision stump (decision tree with depth 1) greedily, which feature should we choose? **Write down your answer in the Markdown cell below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c4202e9f4e5a2074deb181ae4002b6d3",
     "grade": true,
     "grade_id": "cell-db1dde1ecdde83ee",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra credit [5 points]**: You now have all the ingredients to build a decision tree recursively. You can build a decision tree of depth two and report its classification error on the training data and the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0081bf00cc2f9d86f89ee095b3330a7a",
     "grade": false,
     "grade_id": "cell-44230b1deea50132",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### [35 points] Problem 2- KNN for Handwritten Digit Recognition \n",
    "***\n",
    "\n",
    "In this problem you'll implement a K-Nearest Neighbor framework to take an image of a handwritten digit and predict which digit it corresponds to.  \n",
    "\n",
    "![Samples of Handwritten Digits](figs/mnist.png \"MNIST Digits\")\n",
    "\n",
    "To keep run times down we'll only consider the subset of the MNIST data set consisting of the digits $3, 7, 8$ and $9$. \n",
    "\n",
    "**Part A [6 points]**: Executing the following cells will load training and validation data and plot an example handwritten digit.  Explore the training and validation sets and answer the following questions: \n",
    "\n",
    "- How many pixels are in each image in the data set?  \n",
    "- How many examples are there from each class in the training set? \n",
    "- How many examples are there from each class in the validation set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f7f6f7ba21c97621ddcc7fd35fca110a",
     "grade": false,
     "grade_id": "cell-33dd48d1911a6577",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = pickle.load(gzip.open(\"data/mnist21x21_3789.pklz\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1a297399d639ff40d7083d6b4d376af0",
     "grade": false,
     "grade_id": "cell-748091ebd0ea1964",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def view_digit(x, label=None):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    plt.imshow(x.reshape(21,21), cmap='gray');\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "    if label: plt.xlabel(\"true: {}\".format(label), fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "04265b6573e9505a3e07f21c6bca2df6",
     "grade": false,
     "grade_id": "cell-8154afdfafb72531",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "training_index = 0\n",
    "view_digit(X_train[training_index], y_train[training_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "9cdaefb1113d5dba14e150b6d52f1f4b",
     "grade": true,
     "grade_id": "cell-50674b47f654cd5f",
     "locked": false,
     "points": 6,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Write code for answering the questions in Part A and then put your answer in the Markdown cell below.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b50c3b1db910c49ecb8650de370f042",
     "grade": true,
     "grade_id": "cell-738b21d7f854fdb0",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "de2558bb457088791b2d5eebeb1a9ed1",
     "grade": true,
     "grade_id": "cell-ea6def68186d971b",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    \"\"\"\n",
    "    Class to store data for regression problems \n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, y_train, K=5, distance_weighted=False):\n",
    "        \"\"\"\n",
    "        Creates a kNN instance\n",
    "\n",
    "        :param X_train: Training data input in 2D ndarray \n",
    "        :param y_train: Training data output in 1D ndarray \n",
    "        :param K: The number of nearest points to consider in classification\n",
    "        :param distance_weighted: Bool indicating whether to use distance weighting\n",
    "        \"\"\"\n",
    "        \n",
    "        # Import and build the BallTree on training features \n",
    "        from sklearn.neighbors import BallTree\n",
    "        self.balltree = BallTree(X_train)\n",
    "        \n",
    "        # Cache training labels and parameter K \n",
    "        self.y_train = y_train\n",
    "        self.K = K \n",
    "        \n",
    "        # Boolean flag indicating whether to do distance weighting \n",
    "        self.distance_weighted = distance_weighted\n",
    "        \n",
    "    def majority(self, neighbor_indices, neighbor_distances=None):\n",
    "        \"\"\"\n",
    "        Given indices of nearest neighbors in training set, return the majority label. \n",
    "        Break ties by considering 1 fewer neighbor until a clear winner is found. \n",
    "\n",
    "        :param neighbor_indices: The indices of the K nearest neighbors in self.X_train \n",
    "        :param neighbor_distances: Corresponding distances from query point to K nearest neighbors. \n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "            \n",
    "        \n",
    "    def classify(self, x):\n",
    "        \"\"\"\n",
    "        Given a query point, return the predicted label \n",
    "        \n",
    "        :param x: a query point stored as an ndarray  \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Given an ndarray of query points, return yhat, an ndarray of predictions \n",
    "\n",
    "        :param X: an (m x p) dimension ndarray of points to predict labels for \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B [10 points]**: Modify the class above to implement an Unweighted KNN classifier.  There are three methods that you need to complete: \n",
    "\n",
    "- `predict`: Given an $m \\times p$ matrix of validation data with $m$ examples each with $p$ features, return a length-$m$ vector of predicted labels by calling the `classify` function on each example. \n",
    "- `classify`: Given a single query example with $p$ features, return its predicted class label as an integer using KNN by calling the `majority` function. \n",
    "- `majority`: Given an array of indices into the training set corresponding to the $K$ training examples that are nearest to the query point, return the majority label as an integer.  If there is a tie for the majority label using $K$ nearest neighbors, reduce $K$ by 1 and try again.  Continue reducing $K$ until there is a winning label. \n",
    "\n",
    "**Notes**: \n",
    "- Don't even think about implementing nearest-neighbor search or any distance metrics yourself.  Instead, go read the documentation for Scikit-Learn's [BallTree](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html) object.  You will find that its implemented [query](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree.query) method can do most of the heavy lifting for you. \n",
    "- Do not use Scikit-Learn's KNeighborsClassifier in this problem.  We're implementing this ourselves. \n",
    "- You don't need to worry about the `distance_weighted` flag until **Part C**, but we recommend reading ahead a bit. It might be good to think about your implementation of **Part C** before implementing **Part B**. \n",
    "- When you think you're done, execute the following cell to run 4 unit tests based on the example starting on Slide 24 of the [KNN Lecture](https://chenhaot.com/courses/csci4622/slides/lesson05.pdf).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9081907632db400563183b1db77f8afe",
     "grade": false,
     "grade_id": "cell-798d57fabb83fe1b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%run -i tests/tests.py \"prob 2A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "80d8f3b455e42e580eab9afc54fecb08",
     "grade": false,
     "grade_id": "cell-ea5a98439cad3d65",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Part C [5 points]**: Modify the `KNN` class to perform the distance-weighted KNN classification.\n",
    "The so-called Distance-Weighted KNN classifier assigns weights to the the nearest-neighbor training examples proportional to the inverse-distance from the training example to the query point.  Classification is performed by summing the weights associated with each class and predicting the class with the highest weighted-majority vote.  Mathematically we might describe the weighted-vote for a class $c$ as \n",
    "\n",
    "$$\n",
    "\\textrm{Weighted-Vote}(c) = \\displaystyle\\sum_{i \\in {\\cal N}_K} I(y_i = c) \\times \\dfrac{1}{\\|{\\bf x}_i - {\\bf x}\\|}\n",
    "$$\n",
    "\n",
    "A word of caution: it's certainly possible that a query point could be distance $0$ away from some training example.  If this happens your implementation should handle it gracefully and return the appropriate class label.   \n",
    "\n",
    "When you think you're done, execute the following cell to run three final unit tests corresponding to the example on Slide 43 of the [KNN Lecture](https://chenhaot.com/courses/csci4622/slides/lesson05.pdf). Make sure that the changes you make in **Part C** do not affect the unit tests from **Part B**.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3d82acc93dd2482b9b11fb2dad072f38",
     "grade": false,
     "grade_id": "cell-3949b9f6b901e47a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%run -i tests/tests.py \"prob 2B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D [7 points]**: Use your `KNN` class to perform Unweighted KNN on the validation data with $K=3$ and do the following: \n",
    "\n",
    "- **[2 points]** Create a **confusion matrix** (feel free to use the Scikit-Learn [confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "29250910d846e90518c035d8f5c5b3c8",
     "grade": true,
     "grade_id": "cell-4bdb9c57c0f418e2",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "knn = KNN(X_train, y_train, K=3, distance_weighted=True)\n",
    "yhat_valid = knn.predict(X_valid)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a093ae63ff1702589cafc762fd4b85b8",
     "grade": false,
     "grade_id": "cell-2b9ba29fce61478c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "- **[2 points]** Based on your confusion matrix, which digits seem to get confused with other digits the most? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1c1db39e97784b55e28384ad058367b9",
     "grade": true,
     "grade_id": "cell-796deb7245c9b7b7",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "156105d8f73b7c4782d401c04d2f5d87",
     "grade": false,
     "grade_id": "cell-5134547bbf6d382e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "\n",
    "- **[3 points]** Find one misclassified validation example and plot it with the `view_digit` function along with plots of its three nearest neighbors in the training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "dcebbe203ec37e8edd3b24a2334b9234",
     "grade": true,
     "grade_id": "cell-6d2dafb55f88cc7c",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E [7 points]**: **[4 points]** Create a plot of the accuracy of both Unweighted and Distance-Weighted KNN on the validation set on the same set of axes for $K=1, 2, \\ldots, 20$ (feel free to go out to $K=30$ if your implementation is efficient enough to allow it).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "2dc65c059ffb39afb7afba922268b2e2",
     "grade": true,
     "grade_id": "cell-173f9cb33d3db963",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "acc = []\n",
    "wacc = []\n",
    "allks = range(1,30)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "    \n",
    "fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,7))\n",
    "ax.plot(allks, acc, marker=\"o\", color=\"steelblue\", lw=3, label=\"unweighted\")\n",
    "ax.plot(allks, wacc, marker=\"o\", color=\"green\", lw=3, label=\"weighted\")\n",
    "ax.set_xlabel(\"number neighbors\", fontsize=16)\n",
    "ax.set_ylabel(\"accuracy\", fontsize=16)\n",
    "ax.legend(loc=\"upper right\")\n",
    "plt.xticks(range(1,31,2))\n",
    "ax.grid(alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9b0816214111b10b3c20b309f0d7d51c",
     "grade": false,
     "grade_id": "cell-fc4bc58f96c7bf69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**[4 points]** Based on the plot, answer the following questions: \n",
    "\n",
    "- For general $K$, does Unweighted or Weighted KNN appear to perform better? \n",
    "- Which value of $K$ attains the best accuracy on the validation set? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "edde026b64374dd536303a5b9501c3e5",
     "grade": true,
     "grade_id": "cell-7ee1e188129c22ad",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6aabf1ef370badc71b291d0bba19f9e9",
     "grade": false,
     "grade_id": "3intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### [30 points] Problem 3 - Polynomial Regression  and the Bias-Variance Trade-Off\n",
    "***\n",
    "\n",
    "In this problem you will use polynomial regression to explore the Bias-Variance Trade-Off. Assume that our data comes from a model of the form \n",
    "\n",
    "$$Y = f(X) + \\epsilon ~~\\textrm{ where }~~ \\epsilon \\sim N(0,\\sigma^2)$$ \n",
    "\n",
    "For our experiments we'll use $f(x) = \\sin(\\pi x)$. The following functions can be used to generate data from this distribution:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_f(x):\n",
    "    \"\"\"\n",
    "    Returns sin(pi*x) for array of x values\n",
    "    \n",
    "    :@param x: ndarray of feature values\n",
    "    \"\"\"\n",
    "    return np.sin(np.pi * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2af47cfa02b4ebd017c656c6a951126e",
     "grade": false,
     "grade_id": "3aq1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Part A [6 points]**: **[3 points]** Generate a sample of size $n=20$ for $x$-values chosen from a uniform distribution between $0$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "43f4a4b0a8f883bb53e60102c70d80c3",
     "grade": true,
     "grade_id": "3aa1",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_y(x, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Returns y values based on the data generation process.\n",
    "    Note the noise term.\n",
    "    \n",
    "    :@param x: ndarray of feature values\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "def sample(n, sigma=1.0):\n",
    "    \"\"\"Return tuples of data (x, y) that come from \n",
    "    the data generation process.\"\"\"\n",
    "    x = np.random.uniform(0, 1, size=n)\n",
    "    y = get_y(x, sigma=sigma)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "57c5f86cd9d6bd94b429684d981817b4",
     "grade": false,
     "grade_id": "3aq2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**[3 points]**: Make a scatter plot of the data overlayed with the curve of the true function $f(x)=\\sin(\\pi x)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "3bcb4bd020d34168ddfaf848452d9d96",
     "grade": true,
     "grade_id": "3aa2",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "n = 20\n",
    "x, y = sample(n, sigma=.15)\n",
    "xplot = np.linspace(0, 1, 200)\n",
    "fplot = get_f(xplot)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7,7))\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d999fa8d2be8e4ad953704fa68b576a8",
     "grade": false,
     "grade_id": "3bq1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Part B [6 points]**: **[3 points]** Next we need to create a function that can fit a polynomial model to training data and make predictions for unseen data.  Complete the function `poly_predict` below to accomplish this.  Note that there are many ways to do this in Python.  Later this week we'll look into doing this with Scikit-Learn.  You're free to implement it using Scikit-Learn, but you might want to look into Numpy's polynomial fitting functions, [polyfit](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.polyfit.html) and [polyval](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.polyval.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "ad18f73402f92e0d9983d4e9b899e137",
     "grade": true,
     "grade_id": "3ba1",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def poly_predict(x_train, y_train, x_valid, deg):\n",
    "    \"\"\"\n",
    "    Function to train polynomial regression model on training data\n",
    "    and then return a vector of predictions on validation data\n",
    "    \n",
    "    :@param x_train: vector of training features\n",
    "    :@param y_train: vector of training responses\n",
    "    :@param x_valid: vector of validation features to make predictions with\n",
    "    :@param deg: degree of the polynomial model \n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a224c6c352c70260343437c56f7cae9e",
     "grade": false,
     "grade_id": "3bq2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**[3 points]**: Demonstrate that your function is working by using it to generate a plot that include the true function $f(x)$, a scatter plot of your training data, and the curve representing your fitted model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "b1a79919f2aa7437adc2a320f8c677f0",
     "grade": true,
     "grade_id": "3ba2",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "model_plot = poly_predict(x, y, xplot, 2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7,7))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bfa20c8d312c097f7187a0ecdca86d78",
     "grade": false,
     "grade_id": "3c1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Part C [12 points]**: Our goal now will be to make a plot of the decomposition of the expected validation MSE into it's constituent parts.  Recall that we showed in lecture that the expected test MSE can be written as \n",
    "\n",
    "$$\n",
    "\\textrm{E}\\left[\\left(y_0 - \\hat{f}(x_0) \\right)^2\\right] = \\left[~f(x_0) - \\textrm{E}[~\\hat{f}(x_0)~] \\right]^2\n",
    "+ \\textrm{E}\\left[ ~ \\textrm{E}[~\\hat{f}(x_0)]-\\hat{f}(x_0)~\\right]^2 + \\textrm{Var}(\\epsilon)\n",
    "= \\left[\\textrm{Bias}(~\\hat{f}(x_0)\\right]^2 + \\textrm{Var}(~\\hat{f}(x_0)~) + \\textrm{Var}(\\epsilon)\n",
    "$$\n",
    "\n",
    "where $x_0$ represents unseen validation data. We're going to run simulations to estimate $\\left[\\textrm{Bias}(~\\hat{f}(x_0)\\right]^2$ and $\\textrm{Var}(~\\hat{f}(x_0)~)$ for different polynomial models and then plot them against estimates of the true validation MSE. The function `bias_variance_study` below will help you do this.  You just need to fill in a few key parts.  At the end of the day, you should have a plot that looks like this for certain choices of the simulation parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40fb613d42d9f067894c05e68ccc36e1",
     "grade": false,
     "grade_id": "3c2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The $\\textrm{Bias}^2$ term is given by \n",
    "\n",
    "$$\n",
    "\\left[\\textrm{Bias}(~\\hat{f}(x_0)\\right]^2 = \\left[~f(x_0) - \\textrm{E}[~\\hat{f}(x_0)~] \\right]^2\n",
    "$$\n",
    "\n",
    "The first term inside the square is simply the true function $f$ evaluated on the validation data.  The second term inside the square, $\\textrm{E}[~\\hat{f}(x_0)~]$, is the expected value of all estimated models evaluated on the validation data.  We can estimate this by sampling many many training sets, fitting models, evaluating them on many validation sets, and then taking the average. Complete the `squared_bias` function below to do this computation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8228f64d449a15e07a9154d32596b914",
     "grade": false,
     "grade_id": "cell-7a6e91c11ac651f4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The next step is to estimate the $\\textrm{Variance}$.  Let's unpack it \n",
    "\n",
    "$$\n",
    "\\textrm{E}\\left[ ~ \\hat{f}(x_0) - \\textrm{E}[~\\hat{f}(x_0)]~\\right]^2\n",
    "$$\n",
    "\n",
    "Note that $\\textrm{E}[~\\hat{f}(x_0)]$ is the `mean_model` we got from the `squared_bias` function. The other term, $\\hat{f}(x_0)$, is a model estimated on a random training set, and then evaluated on the validation set.  Since we're wrapping this whole thing in an expectation, we're going to sample many many training sets, estimate $\\hat{f}(x_0)$, and then compute the mean squared deviation between these and the `mean_model`.  You will add code to the function below to accomplish this. \n",
    "\n",
    "We'll also estimate the validation MSE so we can compare this to the bias-variance decomposition, but this portion of the code is completed for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "586805337f8a380fa95922a054c536c5",
     "grade": true,
     "grade_id": "cell-faed4049ca36c7ae",
     "locked": false,
     "points": 12,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def bias_variance_study(num_data, num_sims, train_frac=0.8, sigma=0.4, max_deg=10, random_state=1234):\n",
    "    '''\n",
    "    Function to run simulations and estimate the squared-bias, variance, and \n",
    "    validation error of polynomial regression models. \n",
    "    \n",
    "    :@param num_data: number of points in simulated data set \n",
    "    :@param num_sims: number of simulations to run \n",
    "    :@param train_frac: fraction of total data in training set\n",
    "    :@param sigma: standard deviation of noise in data \n",
    "    :@param max_deg: largest degree polynomial to analyze\n",
    "    :@param random_state: seed for random state, for reproducibility\n",
    "    '''\n",
    "    # Set random seed \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # array of polynomial degrees\n",
    "    degrees = range(1, max_deg + 1)\n",
    "    \n",
    "    # train/valid split sizes \n",
    "    num_train = int(np.ceil(train_frac * num_data))\n",
    "    num_valid = num_data - num_train\n",
    "    \n",
    "    # Generate grid of features and shuffle them \n",
    "    x_grid = np.linspace(-1, 1, num_data)\n",
    "    np.random.shuffle(x_grid)\n",
    "    \n",
    "    # Spit into training and validation sets \n",
    "    x_train = x_grid[:num_train]\n",
    "    x_valid = x_grid[num_train:]\n",
    "    \n",
    "    # Dictionaries for predictions and MSE measurements\n",
    "    # Keys are polynomial degree. Each prediction is column \n",
    "    # of nValid x num_sims array. Each mse is entry in vector\n",
    "    # of length num_sims \n",
    "    y_hat_valid = {deg: np.zeros((num_valid, num_sims)) for deg in degrees}\n",
    "    mses_valid = {deg: np.zeros(num_sims) for deg in degrees}\n",
    "    \n",
    "    # Loop over num_sims simulated data sets\n",
    "    for sim in range(num_sims):\n",
    "        \n",
    "        # Generate training and validation responses \n",
    "        y_train = get_y(x_train, sigma=sigma)\n",
    "        y_valid = get_y(x_valid, sigma=sigma)\n",
    "        \n",
    "        # Loop over polynomial degree.  Use function from part B \n",
    "        # to fit to training set and predict on validation set.\n",
    "        # Store predictions those predictions in yHatValid[deg]\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    # Loop over each polynomial degree and compute squared-bias, variance, \n",
    "    # and mean MSE on validation set.  \n",
    "    bias_squared, variance, mse_valid = np.zeros(max_deg), np.zeros(max_deg), np.zeros(max_deg) \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "    # Plot squared bias, variance, and validation MSE \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "    ax.plot(degrees, bias_squared, color=\"steelblue\", lw=3, label=\"Bias**2\")\n",
    "    ax.plot(degrees, variance, color=\"#a76c6e\", lw=3, label=\"Variance\")\n",
    "    ax.plot(degrees, mse_valid, color=\"black\", lw=3, label=\"Valid MSE\")\n",
    "    ax.grid(alpha=0.25)\n",
    "    ax.set_xlabel(\"Model Complexity (Poly Degree)\", fontsize=16)\n",
    "    ax.set_ylabel(\"Error\", fontsize=16)\n",
    "    ax.legend(loc=\"upper left\", fontsize=12)\n",
    "    \n",
    "    # Return arrays of squared-Bias, variance, and MSE \n",
    "    return bias_squared, variance, mse_valid\n",
    "    \n",
    "        \n",
    "bias_squared, variance, mse_valid = bias_variance_study(30, 500, train_frac=0.80,\n",
    "                                                        sigma=0.4, max_deg=12,\n",
    "                                                        random_state=1241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "301ab0196dec8a2b40c96ef5868735b1",
     "grade": false,
     "grade_id": "3d-q",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Part D [6 points]**: When everything is working, increase the size of the simulated data sets and the number of simulation runs and compare the difference between the validation MSE and the sum of the squared-Bias the Variance for each polynomial degree.  Try this for several values of the standard deviation of the model noise.  What do you notice?  How can you explain this result using the notions of Bias-Variance and Reducible and Irreducible error discussed in class?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9ac326d79eb0f869d2dc9b595de1991d",
     "grade": true,
     "grade_id": "3d-a",
     "locked": false,
     "points": 6,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "329e31eba6e45a9fb25a2021a5fac560",
     "grade": false,
     "grade_id": "syllabusquiz",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### [5 points] Problem 4: Syllabus quiz\n",
    "***\n",
    "\n",
    "Please read the [syllabus](https://github.com/BoulderDS/CSCI-4622-Machine-Learning-18fa/blob/master/info/schedule.md) carefullly and finish the [Syllabus quiz](https://goo.gl/forms/pRv3hYrzsUV33OYL2). You do not need to answer anything here. The Markdown cell is only for grading convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3d6cfe1a807ce7839a8d89c76f278223",
     "grade": true,
     "grade_id": "cell-bda8fadfb14fc199",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2cd04f1166e4c7dc0896b2be442644c4",
     "grade": false,
     "grade_id": "coursesurvey",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Optional survey.\n",
    "***\n",
    "\n",
    "We are always interested in your feedback. At the end of each homework, there is a simple anonymous feedback [survey](https://goo.gl/forms/4d9qMlQGYbqq1uNX2) to solicit your feedback for how to improve the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

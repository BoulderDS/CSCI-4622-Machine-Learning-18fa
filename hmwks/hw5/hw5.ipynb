{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: PCA and Boosting\n",
    "\n",
    "\n",
    "This assignment is due on Moodle by **11:59pm on Friday November 16**. \n",
    "Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.\n",
    "Your solutions to computational questions should include any specified Python code and results \n",
    "as well as written commentary on your conclusions.\n",
    "Remember that you are encouraged to discuss the problems with your instructors and classmates, \n",
    "but **you must write all code and solutions on your own**. For a refresher on the course **Collaboration Policy** click [here](https://github.com/BoulderDS/CSCI-4622-Machine-Learning-18fa/blob/master/info/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc.\n",
    "- **Unzip the files in data folder**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[40 points] Problem 1 - Principal Component Analysis\n",
    "---\n",
    "\n",
    "In this problem you'll be implementing dimensionality reduction using the Principal Component Analysis technique. \n",
    "\n",
    "The gist of PCA Algorithm to compute principal components is as follows:\n",
    "- Calculate the covariance matrix X of data points.\n",
    "- Calculate eigenvectors and their corresponding eigenvalues.\n",
    "- Sort the eigenvectors according to their eigenvalues in decreasing order.\n",
    "- Choose first k eigenvectors which satisfies the target explained variance.\n",
    "- Transform the original n dimensional data points into k dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skeleton for the *PCA* class is below. Scroll down to find more information about your tasks as well as unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "794cc99f3b7d4279d779dd191b40e827",
     "grade": false,
     "grade_id": "cell-34ffc5741eeb12ad",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, target_explained_variance=None):\n",
    "        \"\"\"\n",
    "        explained_variance: float, the target level of explained variance\n",
    "        \"\"\"\n",
    "        self.target_explained_variance = target_explained_variance\n",
    "        self.feature_size = -1\n",
    "\n",
    "    def standardize(self, X):\n",
    "        \"\"\"\n",
    "        standardize features using standard scaler\n",
    "        :param m X n: features data\n",
    "        :return: standardized features\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_mean_vector(self, X_std):\n",
    "        \"\"\"\n",
    "        compute mean vector\n",
    "        :param X_std: data\n",
    "        :return n X 1 matrix: mean vector\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_cov(self, X_std, mean_vec):\n",
    "        \"\"\"\n",
    "        Covariance using mean, (don't use any numpy.cov)\n",
    "        :param X_std:\n",
    "        :param mean_vec:\n",
    "        :return n X n matrix:: covariance matrix\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_eigen_vector(self, cov_mat):\n",
    "        \"\"\"\n",
    "        Eigenvector and eigen values using numpy\n",
    "        :param cov_mat:\n",
    "        :return: (eigen_vector,eigen_values)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_explained_variance(self, eigen_vals):\n",
    "        \"\"\"\n",
    "        sort eigen values and compute explained variance.\n",
    "        explained variance informs the amount of information (variance)\n",
    "        can be attributed to each of  the principal components.\n",
    "        :param eigen_vals:\n",
    "        :return: explained variance.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def cumulative_sum(self, var_exp):\n",
    "        \"\"\"\n",
    "        return cumulative sum of explained variance.\n",
    "        :param var_exp: explained variance\n",
    "        :return: cumulative explained variance\n",
    "        \"\"\"\n",
    "        return np.cumsum(var_exp)\n",
    "\n",
    "    def compute_weight_matrix(self, eig_pairs, cum_var_exp):\n",
    "        \"\"\"\n",
    "        compute weight matrix of top principal components conditioned on target\n",
    "        explained variance.\n",
    "        (Hint : use cumilative explained variance and target_explained_variance to find\n",
    "        top components)\n",
    "        \n",
    "        :param eig_pairs: list of tuples containing eigenvector and eigen\n",
    "        values\n",
    "        :param cum_var_exp: cumulative expalined variance by features\n",
    "        :return: weight matrix\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def transform_data(self, X_std, matrix_w):\n",
    "        \"\"\"\n",
    "        transform data to subspace using weight matrix\n",
    "        :param X_std: standardized data\n",
    "        :param matrix_w: weight matrix\n",
    "        :return: data in the subspace\n",
    "        \"\"\"\n",
    "        return X_std.dot(matrix_w)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        entry point to the transform data to k dimensions\n",
    "        standardize and compute weight matrix to transform data.\n",
    "        :param   m X n dimension: train samples\n",
    "        :return  m X k dimension: subspace data.\n",
    "        \"\"\"\n",
    "    \n",
    "        self.feature_size = X.shape[1]\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return self.transform_data(X_std=X_std, matrix_w=matrix_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9954c666813a1b17e1e896c99355318a",
     "grade": false,
     "grade_id": "cell-f115fedc1c3aac43",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**[ PART A - 25 Points]** Your task involves implementing helper functions to compute *mean, \n",
    "covariance, eigenvector and weights*.\n",
    "\n",
    "Complete *fit()* to using all helper functions to find reduced dimension data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "13c2f8825dfdbe5a5a20254d4c808c7c",
     "grade": true,
     "grade_id": "cell-1395ce0d605dc74a",
     "locked": true,
     "points": 25,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%run -i tests/tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ PART A - Continued ] **  Run PCA on *fashion mnist dataset* to reduce the dimension of the data.\n",
    "\n",
    "fashion mnist data consists of samples with *784 dimensions*.\n",
    "\n",
    "Report the reduced dimension $k$ for target explained variance of **0.99**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open('./data/mnist/train_images.pkl','rb'))\n",
    "y_train = pickle.load(open('./data/mnist/train_image_labels.pkl','rb'))\n",
    "X_train = X_train[:15000]\n",
    "y_train = y_train[:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_handler = PCA(target_explained_variance=0.99)\n",
    "X_train_updated = pca_handler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Part B **] Run *SVM Classifier* (refer to HW4) on the reduced dimension data with linear kernel and C = 0.01.\n",
    "\n",
    "Report the accuracy on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_t, X_test, y_t, y_test = train_test_split(X_train_updated, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a1e3c0aee9c27884fa3370d9620b8d50",
     "grade": true,
     "grade_id": "cell-55ac4663a6d818e1",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[PART C 10 Points]** Repeat the same experiment for different values of target explained variance between : **[0.8-1.0]** with increments of 0.04 and provide the reduced dimension size for each.\n",
    "\n",
    "- plot the graph of accuracy vs target explained variance\n",
    "- plot the graph of accuracy vs number of components.\n",
    "\n",
    "Discuss your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0972090884076f81cee8509c090d5bf9",
     "grade": true,
     "grade_id": "cell-d1c639ba61115892",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "for target_variance in np.arange(0.5,1.02,.04):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    print(X_t.shape, target_variance, clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[20 points] Problem 2 :  Statistical PCA for non-zero mean random variables. \n",
    "---\n",
    "Following the notations in class. We define *Principal components* of $x$ as $v_i$. Assuming that the eigenvalues of the covariance matrix $C$ are different from each other.\n",
    "\n",
    "Show that\n",
    "\n",
    "1) $v_1$ is the eigenvector of $C$ corresponding to its largest eigenvalue.\n",
    "\n",
    "2) $v_2^T v_1$ = 0 and $v_2$ is the eigenvector of $C$ corresponding to its second largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5964545d9ca55b8027aacfc1d212e96a",
     "grade": true,
     "grade_id": "cell-35f08a8505587ec1",
     "locked": false,
     "points": 20,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[40 points] Problem 3  Decision Tree Ensembles - Bagging and Boosting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[PART A 20 Points] *House price prediction* using Decision tree ensembles**\n",
    "\n",
    "In this Regression problem, we compare Decision trees and its ensembles - bagging and Boosting on House Price prediction dataset: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "\n",
    "Make use of standard Regression API's of  Decision tree ensembles from sklearn to predict the house price. http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble\n",
    "\n",
    "Complete the ensemble_test class to fit appropriate model recieved as parameter and store the test performance.\n",
    "\n",
    "Use the **ensemble_test** class to plot score and time taken to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "311e5fe3f6a0101509290386cc473eac",
     "grade": false,
     "grade_id": "cell-2f5b11c37751fad9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.metrics import explained_variance_score, precision_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ensemble_test:\n",
    "    \"\"\"\n",
    "        Test multiple model performance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"\n",
    "        initialize data and type of problem\n",
    "        :param X_train:\n",
    "        :param y_train:\n",
    "        :param X_test:\n",
    "        :param y_test:\n",
    "        :param type: regression or classification\n",
    "        \"\"\"\n",
    "        self.scores = {}\n",
    "        self.execution_time = {}\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def fit_model(self, model, name):\n",
    "        \"\"\"\n",
    "        Fit the model on train data.\n",
    "        predict on test and store score and execution time for each fit.\n",
    "        :param model: model\n",
    "        :param name: name of model\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "\n",
    "    def print_result(self):\n",
    "        \"\"\"\n",
    "            print results for all models trained and tested.\n",
    "        \"\"\"\n",
    "\n",
    "        models_cross = pd.DataFrame({\n",
    "            'Model'         : list(self.scores.keys()),\n",
    "            'Score'         : list(self.scores.values()),\n",
    "            'Execution time': list(self.execution_time.values())})\n",
    "        print(models_cross.sort_values(by='Score', ascending=False))\n",
    "\n",
    "    def plot_metric(self):\n",
    "        \"\"\"\n",
    "         plot each metric : time, accuracy\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pickle.load(open('./data/house_predictions/test_train.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a handler for ensemble_test , use the created handler for fitting different models.\n",
    "ensemble_handler = ensemble_test(X_train,y_train,X_test,y_test)\n",
    "from sklearn.tree  import DecisionTreeRegressor\n",
    "decision = DecisionTreeRegressor()\n",
    "ensemble_handler.fit_model(decision, 'decision_tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the cells below to fit the dataset using  *RandomForestRegressor* and *AdaBoostRegressor* with appropriate parameter. (use same n_estimators=500 for both)\n",
    "\n",
    "Compare the accuracy and time taken, report the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3233f586331ad0c1a6a23ddb772a1fc7",
     "grade": true,
     "grade_id": "cell-5f2dd4d130d9a1a8",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "749dc0fc8da3f7e68a8fa9c051f557af",
     "grade": true,
     "grade_id": "cell-56a00ff76f4b97ba",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report Accuracy and Execution time by the model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "30d5c37fa71766f5bddc94a167faf44b",
     "grade": true,
     "grade_id": "cell-ecde0a3f23089aaf",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_handler.plot_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[PART B 15 points]** This is an extension of HW4 problem on sentiment classification over reviews.\n",
    "\n",
    "Here we make use DecisionTree ensembles to **classify** review as positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews  = pd.read_csv('./data/reviews.csv')\n",
    "train, test = train_test_split(reviews, test_size=0.2, random_state=4622)\n",
    "X_train = train['reviews'].values\n",
    "X_test = test['reviews'].values\n",
    "y_train = train['sentiment']\n",
    "y_test = test['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish the following : \n",
    "\n",
    "* create pipeline for *RandomForestClassifier* and *AdaBoostClassifier* as shown for *DecisionTreeClassifier* below. refer : http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble\n",
    "* fit the reviews dataset on the above models and report the results. (tune parameters of classifier for optimal performance)\n",
    "* use n_estimators = 500 for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer and create pipeline.\n",
    "def tokenize(text): \n",
    "    tknzr = TweetTokenizer()\n",
    "    return tknzr.tokenize(text)\n",
    "en_stopwords = set(stopwords.words(\"english\")) \n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words = en_stopwords,\n",
    "    min_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a handler for ensemble_test , use the created handler for fitting different models.\n",
    "ensemble_classifier_handler = ensemble_test(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "pipeline_decision_tree = make_pipeline(vectorizer, DecisionTreeClassifier())\n",
    "ensemble_classifier_handler.fit_model(pipeline_decision_tree, 'decision tree classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_classifier_handler.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "36bfba82604c3aef47f990ead5202ab3",
     "grade": true,
     "grade_id": "cell-5ba823d25c616289",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c1db4b5c2b0ca06fdc563f968143a202",
     "grade": true,
     "grade_id": "cell-0921e901522a923e",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  RESULTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d1cd14cfca55629df4404f2dcbb9c626",
     "grade": true,
     "grade_id": "cell-c19776c62148ed3f",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_classifier_handler.plot_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[PART C  5 points]** Discuss atleast one advantage and disadvantage for *Random Forest\n",
    "*and *Adaboost* in the following space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fb6bbae76b546e466270a1f4c90139da",
     "grade": true,
     "grade_id": "cell-15ec51992550c8fc",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

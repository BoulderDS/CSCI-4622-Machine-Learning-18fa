{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Perceptron, Logistic Regression , Stocashtic Gradient Descent\n",
    "\n",
    "\n",
    "\n",
    "This assignment is due on Moodle by **11:59pm on Friday Sep 28**. \n",
    "Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.\n",
    "Your solutions to computational questions should include any specified Python code and results \n",
    "as well as written commentary on your conclusions.\n",
    "Remember that you are encouraged to discuss the problems with your instructors and classmates, \n",
    "but **you must write all code and solutions on your own**. For a refresher on the course **Collaboration Policy** click [here](https://github.com/BoulderDS/CSCI-4622-Machine-Learning-18fa/blob/master/info/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. \n",
    "- Extra credit questions will not make your homework total scores overflow.\n",
    "\n",
    "**Acknowledgment** : Chris Ketelsen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please put your name and cuidentity username.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Name**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [40 points] Problem 1 : Perceptron Training\n",
    "\n",
    "Consider a binary classification problem on following dataset:\n",
    "\n",
    "| x1   | x2         | x3      | y| \n",
    "|:------:|:------------:| :-----------:|---:|\n",
    "|0|0|0|-1|\n",
    "|0|0|1|1|\n",
    "|0|1|0|1|\n",
    "|1|0|0|1|\n",
    "|0|1|1|-1|\n",
    "|1|1|0|-1|\n",
    "|1|0|1|-1|\n",
    "|1|1|1|1|\n",
    "\n",
    "We are going to experiment with the Perceptron algorithm in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 [5 points]** : Complete the *perceptron_train* function and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change - unless needed\n",
    "data = np.array([\n",
    "    [0, 0, 0, 1, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 1, 1, 0, 1],\n",
    "    [0, 1, 0, 0, 1, 0, 1, 1],\n",
    "    [-1, 1, 1, 1, -1, -1, -1, 1]\n",
    "])\n",
    "data = np.transpose(data)\n",
    "# Initialize the weights and bias (note that we use a non-standard initialization here).\n",
    "weight = np.array([0, 0.5, 0.5])\n",
    "bias = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e98286c0108f35d39fdb46b0602103d1",
     "grade": false,
     "grade_id": "cell-583db73f125b72af",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def perceptron_train(data, weight, bias):\n",
    "    \"\"\"\n",
    "    apply transformation and update weights and bias\n",
    "    :type X: array\n",
    "    :type y: int\n",
    "    :type weight: array\n",
    "    :type bias : int\n",
    "    :rtype: weight, bias,number of mistakes\n",
    "    \"\"\"\n",
    "    mistakes_count = 0\n",
    "    for row in data:\n",
    "        X = np.array(row[:3])\n",
    "        y = row[-1]\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return weight, bias, mistakes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, bias, number_of_mistakes = perceptron_train(data, weight, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Continued** Report weight, bias and number of mistakes after one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a4443206bb8030ffda6798560f3358ec",
     "grade": true,
     "grade_id": "cell-71a254a22738d8dd",
     "locked": true,
     "points": 2.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# for grading - ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the perceptron training for 50 epochs with updated weights and report the weight, bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "27a75ba99dbbf0e265cc8ade336eb0b5",
     "grade": false,
     "grade_id": "cell-9162930c9497743a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "for i in range(epochs):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "print(weight, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9fc7e537a12e005eaf453323c97b8d5b",
     "grade": true,
     "grade_id": "cell-959e42c284018b8d",
     "locked": true,
     "points": 2.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# for grading - ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d728b9f90e01d970fd2163eada109dc2",
     "grade": false,
     "grade_id": "cell-fdb42b2a6b9cefea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Part 2 [5 points]:** Is it possible that your Perceptron classifier would \n",
    "    ever perfectly classify all training examples after more passes of the Perceptron Algorithm?\n",
    "    Clearly explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "aab341a9986cebe8f0e3a28dd5841d0e",
     "grade": true,
     "grade_id": "cell-9e4148ad96554da5",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c2f4b7aa7e781494cda5adca66c8c67a",
     "grade": false,
     "grade_id": "cell-91fc1b93bd24f56d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Part 3 [5 points]:** Does the Perceptron classifier necessarily make the same number of mistakes after one epoch if the data is presented in any other randomized order? \n",
    "    Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "008162ff8ebf6900421370575c9e7014",
     "grade": true,
     "grade_id": "cell-47e30b7cd8370b85",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Part 4 [10 Points]: Perceptron Classifier on random generated data:\n",
    "Using the Implementation in Part 1, Update the Perceptron Learning Algorithm to explore the convergence on linearly separable simulated data sets with particular properties. Take a look at the Perceptron class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1f5adaeea25eff984cc7920cbe2014b0",
     "grade": false,
     "grade_id": "cell-7770ef2854032d8b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Class to fit a perceptron classifier to simulated data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=100, margin=0.1, X=None, y=None, random_state=1241):\n",
    "        \"\"\"\n",
    "        Initializes Perceptron class.  Generates training data and sets parameters. \n",
    "\n",
    "        :param n: the number of training examples\n",
    "        :param margin: the margin between decision boundary and data\n",
    "        :param random_state: seed for random number generator \n",
    "        :param X: Input training features.  Only used for unit testing. \n",
    "        :param y: Input training labels.  Only used for unit testing. \n",
    "        \"\"\"\n",
    "        # initalize random seed\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "        # initialize parameters\n",
    "        self.n, self.M = n, margin\n",
    "\n",
    "        # generate random simulated data\n",
    "        self.X_train, self.y_train = self.gen_data()\n",
    "\n",
    "        # only used for unit tests\n",
    "        if X is not None and y is not None:\n",
    "            self.X_train, self.y_train, self.n = X, y, X.shape[0]\n",
    "\n",
    "        # initialize weights and bias to zero\n",
    "        self.w = np.array([1.0, 0.0])\n",
    "        self.b = 0\n",
    "\n",
    "        # initialize total mistake counter\n",
    "        self.num_mistakes = 0\n",
    "\n",
    "    def train(self, max_epochs=100):\n",
    "        \"\"\"\n",
    "        Runs the Perceptron Algorithm until all training data is correctly classified. \n",
    "\n",
    "        :param max_epochs: Maximum number of epochs to perform before stopping.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def plot_model(self, decision_boundary=False):\n",
    "        \"\"\"\n",
    "        Plots the simulated data.  Plots the learned decision boundary (#TODO) \n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n",
    "        colors = [\"steelblue\" if yi == -\n",
    "                  1 else \"#a76c6e\" for yi in self.y_train]\n",
    "        ax.scatter(self.X_train[:, 0], self.X_train[:, 1], color=colors, s=75)\n",
    "        if decision_boundary:\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        ax.grid(alpha=0.25)\n",
    "        ax.set_xlabel(r\"$x_1$\", fontsize=16)\n",
    "        ax.set_ylabel(r\"$x_2$\", fontsize=16)\n",
    "\n",
    "    def gen_data(self):\n",
    "        \"\"\"\n",
    "        Generate random linearly separable data with given margin. \n",
    "        Note: You should not need to change this function \n",
    "        \"\"\"\n",
    "        flip = np.random.choice([-1, 1])\n",
    "        pos_x1 = np.random.uniform(-1 / np.sqrt(2), 1 / np.sqrt(2), int(self.n / 2))\n",
    "        pos_x2 = np.random.uniform(\n",
    "            self.M + flip * 0.1, 1 / np.sqrt(2), int(self.n / 2))\n",
    "        pos_x2[-1] = self.M + flip * 0.1\n",
    "        neg_x1 = np.random.uniform(-1 / np.sqrt(2), 1 / np.sqrt(2), int(self.n / 2))\n",
    "        neg_x2 = np.random.uniform(-1 / np.sqrt(2), -\n",
    "                                   self.M + flip * 0.1, int(self.n / 2))\n",
    "        neg_x2[-1] = -self.M + flip * 0.1\n",
    "        X = np.concatenate((np.column_stack((pos_x1, pos_x2)),\n",
    "                            np.column_stack((neg_x1, neg_x2))))\n",
    "        X = np.dot(X, np.array(\n",
    "            [[np.cos(np.pi / 6), np.sin(np.pi / 6)], [-np.sin(np.pi / 6), np.cos(np.pi / 6)]]))\n",
    "        y = np.array([+1] * int(self.n / 2) + [-1] * int(self.n / 2))\n",
    "        rand_order = np.random.choice(\n",
    "            range(self.n), replace=False, size=self.n)\n",
    "        return X[rand_order], y[rand_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 A**: The Perceptron class above has the capability to generate it's own training data with certain properties. Execute the cell below to generate $n=100$ simulated training examples and plot them. Experiment with the margin parameter (good values to try are between $0.01$ and $0.4$). Explain what the margin parameter is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 0.2 # subject to experiment\n",
    "perc = Perceptron(n=100, margin=margin)\n",
    "perc.plot_model(decision_boundary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "50441211d139b2a4f9da7cd7ab870d31",
     "grade": true,
     "grade_id": "cell-788872a24aff264a",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": false
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 B** : Modify the train method in the Perceptron class to perform the Perceptron Learning Algorithm and learn weights ${\\bf w}$ and bias $b$ that perfectly classify the linearly separable training data. Your implementation should:\n",
    "\n",
    "visit all training examples in a random shuffled order over each training epoch\n",
    "terminate when you perform an entire epoch without making a single classification error on the training data\n",
    "use the self.num_mistakes counter to count the total number of classification errors that are made over the entire training process\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "You should not use Scikit-Learn's Perceptron object in your solution.\n",
    "It's a good idea to implement a stopping criterion based on the max_epochs parameter right away. Later we'll look at training sets that will terminate on their own, but implementing this stop will save you some pain in the development process.\n",
    "Do not change the initial guess for the weights and bias. These values were chosen to match the example done in lecture for the unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b056fb1251d734bc57d5db999405c41f",
     "grade": true,
     "grade_id": "cell-ad1677859c4ce262",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%run -i tests.py \"prob 1A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 C** : Modify the plot_model method so that it plots the learned decision boundary with the training data. Demonstrate that your method is working by training a perceptron with a margin of your choice and displaying the resulting plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bfd198608dd122cc1be160a9766b8d25",
     "grade": true,
     "grade_id": "cell-807e6237e11d03ed",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = Perceptron(n=100, margin=0.2)\n",
    "perc.train()\n",
    "perc.plot_model(decision_boundary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 5 [10 points]** : \n",
    "\n",
    "Verify the theorem which states that, if you train a perceptron on linearly separable training data with margin $M > 0$ and each training point satisfies $\\|{\\bf x}\\|_2 \\leq 1$ then the Perceptron Training Algorithm will complete after making at most $1/M^2$ classification mistakes\n",
    "\n",
    "Do the following to verify the above statement: \n",
    "- train perceptron with $n = 100$ and different margin's ($M=0.3, 0.1, 0.01, 0.001,$ and $0.0001$).\n",
    "- Produce a log-log plot with $1/M$ on the horizontal axis and average number of mistakes on the vertical axis. \n",
    "- On the same set of axes, plot the theoretical upper bound on the number of training mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a51d12ddace6cca4e1138f86b7ac6114",
     "grade": true,
     "grade_id": "cell-5eb12c673017afa0",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Credit [5 points]** : \n",
    "\n",
    "* Explain the limitations of the vanilla perceptron implemented above.\n",
    "* Provide pseudo code/steps to implememt Voting perceptron or Average Perceptron and discuss the pros and cons (Compare space complexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f845d3da91814bdd23a2bf0699bf8fba",
     "grade": true,
     "grade_id": "cell-945d3bde845764bf",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [60 points] Problem 2 : Logistic Regression + SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Problem, you'll implement a Logistic Regression classifier to predict whether person on titanic will survive or not.\n",
    "\n",
    "\n",
    "Dataset has following attributes:\n",
    "\n",
    "|Variable|Definition|Key|\n",
    "|:----:|:----:|:---|\n",
    "|survival|\tSurvival|\t0 = No, 1 = Yes|\n",
    "|pclass|\tTicket class|\t1 = 1st, 2 = 2nd, 3 = 3rd|\n",
    "|sex\t| Sex|\tmale, female|\n",
    "|Age |Age in years\t||\n",
    "|sibsp\t|# of siblings / spouses aboard the Titanic| |\n",
    "|parch\t|# of parents / children aboard the Titanic\t||\n",
    "|fare\t|Passenger fare|\t|\n",
    "|embarked | Port of Embarkation | one hot encoded C = Cherbourg, Q = Queenstown, S = Southampton|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 [5 points]** : Firstly implement the sigmoid function to return the output by applying the sigmoid function to the input\n",
    "parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9f9a07a700b31c2dec3c9c45b18186a6",
     "grade": false,
     "grade_id": "cell-389deffcd0e8b595",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from math import exp, log\n",
    "\n",
    "def sigmoid(score, threshold=20.0):\n",
    "    \"\"\"\n",
    "    Sigmoid function with a threshold\n",
    "    :param score: A real valued number to convert into a number between 0 and 1\n",
    "    :param threshold : Prevent overflow of exp by capping activation at 20.\n",
    "    return sigmoid function result.\n",
    "    \"\"\"\n",
    "    # TODO: Finish this function to return the output of applying the sigmoid\n",
    "    # function to the input score (Please do not use external libraries)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf1575c360183cb51e505d7995300c65",
     "grade": true,
     "grade_id": "cell-73fb14b862081aa9",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#for grading - ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a class to load titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have install pandas and numpy before you run.\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import exp, log\n",
    "from collections import defaultdict\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Class to load dataset containing titanic survival features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, location, random_state=1241):\n",
    "        # You shouldn't have to modify this class, but you can if you'd like\n",
    "        # Load the dataset\n",
    "        np.random.seed(random_state)\n",
    "        f = gzip.open(location, 'rb')\n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = pickle.load(f)\n",
    "        f.close()\n",
    "        print('Train set : ', len(self.train_x))\n",
    "        \n",
    "    @staticmethod\n",
    "    def shuffle(X, y):\n",
    "        \"\"\" Shuffle training data \"\"\"\n",
    "        shuffled_indices = np.random.permutation(len(y))\n",
    "        return X[shuffled_indices], y[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2** :\n",
    "\n",
    "**Part A [15 points]** : Using *sigmoid* function implemented earlier, Finish the *sgd_update* function so that it performs stochastic gradient descent on the single training example and updates the weight vector correspondingly without regularization provided by : \n",
    "\n",
    "$$\n",
    "\\textrm{NLL}({\\bf \\beta}) = -\\displaystyle\\sum_{i=1}^n \\left[y_i \\log \\textrm{sigm}(\\boldsymbol{\\beta}^T{\\bf x})       + (1-y_i)\\log(1 - \\textrm{sigm}(\\boldsymbol{\\beta}^T{\\bf x}))\\right] \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e3617deb2a1f2d3f4d00638c59286a35",
     "grade": true,
     "grade_id": "cell-26bd636741b8b10c",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class LogReg:\n",
    "    def __init__(self, num_features, eta):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "        :param num_features: The number of features (including bias)\n",
    "        :param eta: A function that takes the iteration as an argument (the default is a constant value)\n",
    "        \"\"\"\n",
    "        self.w = np.zeros(num_features)\n",
    "        self.eta = eta\n",
    "\n",
    "    def progress(self, examples_x, examples_y):\n",
    "        \"\"\"\n",
    "        Given a set of examples, compute the probability and accuracy\n",
    "        :param examples: The dataset to score\n",
    "        :return: A tuple of (log probability, accuracy)\n",
    "        \"\"\"\n",
    "\n",
    "        logprob = 0.0\n",
    "        num_right = 0\n",
    "        for x_i, y in zip(examples_x, examples_y):\n",
    "            p = sigmoid(self.w.dot(x_i))\n",
    "            if y == 1:\n",
    "                logprob += math.log(p)\n",
    "            else:\n",
    "                logprob += math.log(1.0 - p)\n",
    "\n",
    "            # Get accuracy\n",
    "            if abs(y - p) <= 0.5:\n",
    "                num_right += 1\n",
    "\n",
    "        return logprob, float(num_right) / float(len(examples_y))\n",
    "\n",
    "    def sgd_update(self, x_i, y, lam = 0.0):\n",
    "        \"\"\"\n",
    "        Compute a stochastic gradient update to improve the log likelihood.\n",
    "        :param x_i: The features of the example to take the gradient with respect to\n",
    "        :param y: The target output of the example to take the gradient with respect to\n",
    "        :param lam : regularization term\n",
    "        :return: Return the new value of the regression coefficients\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Finish this function to do a single stochastic gradient descent update\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return self.w\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i tests.py \"prob 2A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART B [10] points**: Complete the following code below to loop over the training data and perform stochastic gradient descent for the pre-defined number of epoch.\n",
    "\n",
    "Try different combinations of epochs, learning rate and regularization term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO redefine learning rate and epochs accordingly\n",
    "eta  = 1e-3\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "checksum": "045cfbceb81e0edbfa0fe9cbc77ada9d",
     "grade": true,
     "grade_id": "cell-a90025845b0cca0f",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_handler = Dataset('./data/titanic.pklz')\n",
    "lr = LogReg(dataset_handler.train_x.shape[1], eta)\n",
    "print('Train Data :', dataset_handler.train_x.shape)\n",
    "print('Test Data :', dataset_handler.test_x.shape)\n",
    "# Iterations\n",
    "iteration = 0\n",
    "accuracy_array = []\n",
    "for epoch in range(epochs):\n",
    "    # TODO: Finish the code to loop over the training data and perform a stochastic\n",
    "    # gradient descent update on each training example.\n",
    "    # NOTE: It may be helpful to call upon the 'progress' method in the LogReg\n",
    "    # class to make sure the algorithm is truly learning properly on both training and test data\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C [5 points]** Report train and test accuracy for the above experiments after 500 epochs with eta = 1e-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "cd767590869c90c0bde898740f270e2b",
     "grade": true,
     "grade_id": "cell-c906b4528206c03c",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D [5 points]** What is the role of the learning rate (eta) and number of epochs? (Provide plots of train accuracy vs. $\\eta$, train accuracy vs number of epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "a4b1d9f962a2ddedfdf79ec74c657b3c",
     "grade": true,
     "grade_id": "cell-dd0ea201d734cc98",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Part E [10 points]**: Update your implementation of the *sgd_update* method so that it performs regularized SGD updates of the model parameters to minimize the regularized loss function\n",
    "\n",
    "$$\n",
    "\\textrm{Loss}({\\bf \\beta}) = -\\displaystyle\\sum_{i=1}^n \\left[y_i \\log \\textrm{sigm}(\\boldsymbol{\\beta}^T{\\bf x})       + (1-y_i)\\log(1 - \\textrm{sigm}(\\boldsymbol{\\beta}^T{\\bf x}))\\right] + \\lambda\\displaystyle\\sum_{k=1}^p \\beta_k^2\n",
    "$$\n",
    "\n",
    "Note that you should NOT regularize the bias parameter $\\beta_0$.\n",
    "\n",
    "Provide train and test accuracy after above change with lam=1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "309cf1564681e889863b0a88db4d6db7",
     "grade": true,
     "grade_id": "cell-3879c5176d481064",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%run -i tests.py \"prob 2B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F [5 points]** what is the effect of regularization term with respect to accuracy? (Provide plots of accuracy vs. $\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "e229b44ff0f7a79187a3ae58fb598142",
     "grade": true,
     "grade_id": "cell-0ce393d045320da5",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3 [5 points]** \n",
    "\n",
    "Time based Learning Rate is dynamic learning rate given the following equation\n",
    "\n",
    "$LearningRate = \\eta / (1 + decay * current epoch)$\n",
    "\n",
    "Train SGD with dynamic learning rate defined above and provide the following details:\n",
    "* use initial learning rate ($\\eta$) as 0.1.\n",
    "* use decay as 0.001.\n",
    "* Update learning rate every epoch (lr.eta).\n",
    "* plot of Train Accuracy and learning rate for each epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c910a28563590c7c84350b9a42d8eb20",
     "grade": true,
     "grade_id": "cell-c7e6de980a227f54",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e6ad06a30b9fd645e8e8024531069050",
     "grade": true,
     "grade_id": "cell-4cc9c3502840b913",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "eta  = 1e-1\n",
    "epochs = 100\n",
    "dataset_handler = Dataset('./data/titanic.pklz')\n",
    "lr = LogReg(dataset_handler.train_x.shape[1], eta)\n",
    "\n",
    "# Iterations\n",
    "iteration = 0\n",
    "accuracy_array = []\n",
    "for epoch in range(epochs):\n",
    "    # TODO: Finish the code to loop over the training data and perform a stochastic\n",
    "    # gradient descent update on each training example.\n",
    "    # NOTE: It may be helpful to call upon the 'progress' method in the LogReg\n",
    "    # class to make sure the algorithm is truly learning properly on both training and test data\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Optional survey.\n",
    "***\n",
    "\n",
    "We are always interested in your feedback. At the end of each homework, there is a simple anonymous feedback [survey](https://goo.gl/forms/IJWcqugVHCJ6maSJ3) to solicit your feedback for how to improve the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
